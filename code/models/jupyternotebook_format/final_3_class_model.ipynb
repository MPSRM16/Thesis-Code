{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwO283-J6nke"
      },
      "source": [
        "# üß™ MS/MS Spectra Modification Classifier with Transformers\n",
        "\n",
        "This notebook builds and trains a deep learning model designed to detect and classify *post-translational modifications (PTMs)* in MS/MS spectra from shotgun proteomics. The input is MS/MS spectra in `mgf` format and the classifier is based on a hybrid CNN-Transformer architecture.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Objectives\n",
        "\n",
        "- **Multi-class Classification**: If modified, predict the specific type:\n",
        "  - Unmodified\n",
        "  - Oxidation\n",
        "  - Phosphorylation\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Environment Setup\n",
        "\n",
        "The following libraries and paths are configured:\n",
        "\n",
        "#### üì¶ Core Libraries\n",
        "- `torch`, `torch.nn`, `torch.optim`: PyTorch for neural network construction and training.\n",
        "- `numpy`, `random`, `os`, `sys`: Utilities for array operations, randomness, and file handling.\n",
        "- `math`, `datetime`, `logging`: Math functions, timestamping, and logging system.\n",
        "- `matplotlib.pyplot`: (optional) Visualization.\n",
        "- `scikit-learn`: Evaluation metrics and dataset splitting.\n",
        "\n",
        "\n",
        "#### üõ†Ô∏è Path Configuration\n",
        "- Adds the dataset directory on Google Drive to the system path to ensure data files can be accessed during training and evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### üß¨ Pipeline Overview\n",
        "\n",
        "This project includes the following components:\n",
        "- **MGF File Parsing**: Custom loader to extract raw spectra from `.mgf` files.\n",
        "- **Spectral Preprocessing**: Converts spectra into binned, normalized vector representations.\n",
        "- **Metadata Normalization**: Processes and scales parent ion mass (`pepmass`) for model input.\n",
        "- **Transformer-Based Model**: Hybrid neural architecture combining CNNs, self-attention, and metadata fusion.\n",
        "- **Training & Evaluation**: Loop with weighted loss, custom metrics, logging, and model checkpointing.\n",
        "\n",
        "This setup is tailored for high-performance PTM classification while maintaining compatibility with Google Colab workflows and GPU acceleration tuned using Optuna.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìÅ Directory Setup Instructions\n",
        "\n",
        "Before running the notebook, ensure your **Google Drive** is properly structured so that the code can:\n",
        "\n",
        "* Load `.mgf` spectra files.\n",
        "* Save model weights.\n",
        "* Persist log files from training.\n",
        "\n",
        "This is **required** for the notebook to run end-to-end.\n",
        "\n",
        "---\n",
        "\n",
        "### üîó 1. Mount Google Drive\n",
        "\n",
        "At the beginning of your notebook, run:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "You will be prompted to authorize access.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÇ 2. Create This Folder Structure in Your Drive\n",
        "\n",
        "Organize your files inside `MyDrive` as follows:\n",
        "\n",
        "```\n",
        "MyDrive/\n",
        "‚îú‚îÄ‚îÄ data/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ balanced_dataset/                ‚Üê contains balanced .mgf files for training, they dont neeed to be balanced in the class distribution, but it help in tranning performance\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ split_file_001.mgf\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ split_file_002.mgf\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ peak_encoder_transformer_pipeline/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ model_weights/                   ‚Üê for saving trained model weights\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ logs/                            ‚Üê for saving training logs\n",
        "```\n",
        "\n",
        "If these folders don't exist, you can create them manually in Google Drive or use Python:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/data/balanced_dataset\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/peak_encoder_transformer_pipeline/model_weights\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/peak_encoder_transformer_pipeline/logs\", exist_ok=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è 3. Update Paths in the Code (if needed)\n",
        "\n",
        "These variables should point to the correct folders:\n",
        "\n",
        "```python\n",
        "input_dir = \"your split dataset path\"\n",
        "model_weights_dir = \"path for where the weights go\"\n",
        "log_dir = \"path for the log system for the per bath logs to be\"\n",
        "```\n",
        "\n",
        "Make sure the paths you changed to your own are comtable with is expected of each one of them\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Once these are set**, you're ready to run the notebook end-to-end, including training, evaluation, and logging.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L4Mwr_leUvqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "swzS1eYoUtS6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dvfgv2BLQazC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39a11a9-581a-4272-efd0-3ed06479ba3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Set up the enviorment imports and paths that are necessary for the processing of the cells\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('content/drive/MyDrive/data/balanced_dataset')  # Add the folder containing main.py to sys.path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import SiLU\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W4C018WnI44",
        "outputId": "0ea04865-921b-4c32-8914-4efb2a4a081e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJlFhM_A5Mic"
      },
      "source": [
        "## üìÇ DatasetHandler class for loading MGF files\n",
        "\n",
        "This section defines the `DatasetHandler` class responsible for managing the loading and iteration over `.mgf` files containing MS/MS spectra.\n",
        "Loading only one `.mgf` at a time in order to make the pipeline scalable without running avoiding memory memory issues.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ `DatasetHandler` Overview\n",
        "\n",
        "The `DatasetHandler` class provides a memory-efficient way to iterate through `.mgf` files stored in a directory. It supports:\n",
        "\n",
        "- **Shuffling input files** to randomize data order across training loops.\n",
        "- **Per-file usage tracking** with `MAX_FILE_PASSES`, ensuring that no file is overused during training.\n",
        "- **Controlled looping** over the dataset using `num_loops` to allow multiple training epochs without data reloading.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Key Components make this under the code explaining how to use it, make it like an example under evrything\n",
        "\n",
        "#### üîß Initialization\n",
        "```python\n",
        "handler = DatasetHandler(input_dir=\"/path/to/mgf\", num_loops=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0virZyWOPEXP"
      },
      "outputs": [],
      "source": [
        "#Setting up the dataset handler class that handles the input\n",
        "#There is still prints to remove\n",
        "\n",
        "MAX_FILE_PASSES = 1 # Max times a file can be used before being ignored\n",
        "\n",
        "class DatasetHandler:\n",
        "    def __init__(self, input_dir, num_loops=1):\n",
        "        \"\"\"\n",
        "        Initialize the dataset handler.\n",
        "\n",
        "        Args:\n",
        "            input_dir (str): Path to the directory containing split MGF files.\n",
        "            num_loops (int): Number of times the dataset should be iterated.\n",
        "        \"\"\"\n",
        "        self.files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.mgf')]\n",
        "        self.files = random.sample(self.files, len(self.files))  # Shuffle files\n",
        "        self.file_usage_counter = {f: 0 for f in self.files}\n",
        "        self.num_loops = num_loops\n",
        "        self.loop_count = 0\n",
        "\n",
        "    def get_next_file(self) -> list:\n",
        "      \"\"\"\n",
        "      Load one MGF file at a time into RAM and return all valid spectra from it.\n",
        "\n",
        "      Returns:\n",
        "          list of dict: Each dict contains a valid spectrum and its metadata.\n",
        "      \"\"\"\n",
        "      while self.loop_count < self.num_loops:\n",
        "          available_files = [f for f in self.files if self.file_usage_counter[f] < MAX_FILE_PASSES]\n",
        "          if not available_files:\n",
        "              self.loop_count += 1\n",
        "              if self.loop_count < self.num_loops:\n",
        "                  print(\"Restarting dataset loop...\")\n",
        "                  self.file_usage_counter = {f: 0 for f in self.files}\n",
        "                  continue\n",
        "              else:\n",
        "                  print(\"All dataset loops completed.\")\n",
        "                  return None\n",
        "\n",
        "          file = random.choice(available_files)\n",
        "          print(f\"Processing file: {file}\")\n",
        "\n",
        "          spectra = []\n",
        "          spectrum_data = None\n",
        "\n",
        "          with open(file, 'r') as f:\n",
        "              for line in f:\n",
        "                  line = line.strip()\n",
        "\n",
        "                  if line == \"BEGIN IONS\":\n",
        "                      spectrum_data = {\"mz_values\": [], \"intensity_values\": []}\n",
        "\n",
        "                  elif line.startswith(\"TITLE=\") and spectrum_data is not None:\n",
        "                      spectrum_data[\"title\"] = line.split(\"=\", 1)[1].strip()\n",
        "\n",
        "                  elif line.startswith(\"PEPMASS=\") and spectrum_data is not None:\n",
        "                      try:\n",
        "                          spectrum_data[\"pepmass\"] = float(line.split(\"=\", 1)[1].split()[0])\n",
        "                      except ValueError:\n",
        "                          spectrum_data[\"pepmass\"] = None  # mark as missing\n",
        "                  elif line.startswith(\"CHARGE=\") and spectrum_data is not None:\n",
        "                      charge_str = line.split(\"=\", 1)[1].strip()\n",
        "                      match = re.match(r'^(\\d+)', charge_str)  # Match one or more digits at the start\n",
        "                      if match:\n",
        "                          spectrum_data[\"charge\"] = int(match.group(1))\n",
        "                      else:\n",
        "                          print(f\"[SKIPPED CHARGE] Invalid charge format: '{charge_str}'\")\n",
        "                          spectrum_data[\"charge\"] = None\n",
        "\n",
        "                  elif line == \"END IONS\" and spectrum_data is not None:\n",
        "                      title = spectrum_data.get(\"title\", \"\").strip()\n",
        "                      mz_vals = spectrum_data.get(\"mz_values\", [])\n",
        "                      int_vals = spectrum_data.get(\"intensity_values\", [])\n",
        "\n",
        "                      # Final validation before appending\n",
        "                      if not title or not title.strip():\n",
        "                          print(f\"[SKIPPED] Missing TITLE in file: {file}\")\n",
        "                      elif not mz_vals or not int_vals:\n",
        "                          print(f\"[SKIPPED] Empty m/z or intensity in file: {file}\")\n",
        "                      elif len(mz_vals) != len(int_vals):\n",
        "                          print(f\"[SKIPPED] Mismatched m/z and intensity count in file: {file}\")\n",
        "                      elif np.sum(int_vals) == 0:\n",
        "                          print(f\"[SKIPPED] All-zero intensities in spectrum '{title}'\")\n",
        "                      else:\n",
        "                          spectra.append(spectrum_data)\n",
        "\n",
        "                      spectrum_data = None  # Reset for next spectrum\n",
        "\n",
        "                  else:\n",
        "                      if spectrum_data is not None:\n",
        "                          try:\n",
        "                              parts = line.split()\n",
        "                              if len(parts) != 2:\n",
        "                                  raise ValueError(\"Expected two float values\")\n",
        "                              mz, intensity = map(float, parts)\n",
        "                              if math.isnan(mz) or math.isnan(intensity):\n",
        "                                  raise ValueError(\"NaN detected\")\n",
        "                              spectrum_data[\"mz_values\"].append(mz)\n",
        "                              spectrum_data[\"intensity_values\"].append(intensity)\n",
        "                          except ValueError:\n",
        "                              #print(f\"[SKIPPED LINE] Invalid peak: '{line}' in file: {file}\")\n",
        "                              continue\n",
        "\n",
        "          self.file_usage_counter[file] += 1\n",
        "\n",
        "          if spectra:\n",
        "              return spectra, file\n",
        "\n",
        "      print(\"All spectra processed.\")\n",
        "      return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ilh_g_W8mcF"
      },
      "source": [
        "# ‚öôÔ∏è Dense Vector Binning for 1D CNN Input  \n",
        "This section defines the updated preprocessing pipeline for converting annotated MS/MS spectra into dense, fixed-length vectors. These are tailored for use in models such as CNNs or hybrid CNN-Transformer architectures.\n",
        "\n",
        "## üîß Functions:\n",
        "\n",
        "### `bin_spectra_to_dense_vectors`  \n",
        "Converts a list of spectra into fixed-length vectors by:  \n",
        "- **Binning the m/z values** across a specified range (`mz_min` to `mz_max`) into `num_bins`.  \n",
        "- Each bin holds the **intensity sum of peaks** falling into that m/z range.  \n",
        "- Applies **sliding window normalization**:  \n",
        "  The m/z axis is divided into fixed-size windows (e.g., 200 m/z), and intensities within each window are normalized individually to the [0, 1] range. This preserves local signal structure and prevents domination by high-intensity regions.\n",
        "\n",
        "### `process_spectra_with_handler`  \n",
        "Processes a batch of spectra by:  \n",
        "- Logging and skipping spectra with empty or invalid m/z or intensity values.  \n",
        "- Using the above function to apply binning and **sliding window normalization**.  \n",
        "- Skipping spectra with no signal after binning (i.e., zero-vector).  \n",
        "\n",
        "Returns a list of valid, normalized dense vectors for CNN input and logs the total number of skipped spectra.\n",
        "\n",
        "## üì¶ Output Format:  \n",
        "Each spectrum becomes a 1D `np.array` of shape `(num_bins,)` with `float32` values.  \n",
        "\n",
        "The final output is either:  \n",
        "- a stacked `np.ndarray` of shape `(batch_size, num_bins)` when using `bin_spectra_to_dense_vectors` directly on a list, or  \n",
        "- a list of valid vectors (1 per spectrum) when using `process_spectra_with_handler`.\n",
        " used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w484qgSIb7tt"
      },
      "outputs": [],
      "source": [
        "def bin_spectra_to_dense_vectors(spectra_data, num_bins=5000, mz_min=100.0, mz_max=2200.0, window_size=200.0):\n",
        "    \"\"\"\n",
        "    Converts spectra into dense, fixed-length binned vectors suitable for 1D CNN input with sliding window normalization.\n",
        "\n",
        "    Parameters:\n",
        "    - spectra_data: List of spectra dicts with 'mz_values' and 'intensity_values'.\n",
        "    - num_bins: Number of bins to divide the m/z range [mz_min, mz_max] into.\n",
        "    - mz_min: Minimum m/z value for binning.\n",
        "    - mz_max: Maximum m/z value for binning.\n",
        "    - window_size: Size of m/z window for normalization (default is 200.0).\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray of shape (batch_size, num_bins) with per-spectrum normalized intensities.\n",
        "    \"\"\"\n",
        "    bin_edges = np.linspace(mz_min, mz_max, num_bins + 1)\n",
        "    binned_spectra = []\n",
        "\n",
        "    for spectrum in spectra_data:\n",
        "        mz_values = np.array(spectrum['mz_values'])\n",
        "        intensity_values = np.array(spectrum['intensity_values'])\n",
        "\n",
        "        if len(mz_values) == 0 or len(intensity_values) == 0:\n",
        "            binned_spectra.append(np.zeros(num_bins, dtype=np.float32))\n",
        "            continue\n",
        "\n",
        "        # Create an array to hold the binned intensities (fixed size)\n",
        "        binned_intensity = np.zeros(num_bins)\n",
        "\n",
        "        # Iterate over windows of m/z values\n",
        "        for window_start in np.arange(mz_min, mz_max, window_size):\n",
        "            window_end = window_start + window_size\n",
        "            window_mask = (mz_values >= window_start) & (mz_values < window_end)\n",
        "            window_mz_values = mz_values[window_mask]\n",
        "            window_intensity_values = intensity_values[window_mask]\n",
        "\n",
        "            if len(window_mz_values) > 0:\n",
        "                # Bin the intensities for this window\n",
        "                binned_window_intensity, _ = np.histogram(window_mz_values, bins=bin_edges, weights=window_intensity_values)\n",
        "\n",
        "                # Normalize the binned intensities within this window\n",
        "                min_val = binned_window_intensity.min()\n",
        "                max_val = binned_window_intensity.max()\n",
        "                range_val = max_val - min_val if max_val != min_val else 1e-6\n",
        "                normalized_binned_window = (binned_window_intensity - min_val) / range_val\n",
        "\n",
        "                # Add the normalized intensities to the final vector (same size as before)\n",
        "                binned_intensity += normalized_binned_window\n",
        "\n",
        "        binned_spectra.append(binned_intensity.astype(np.float32))\n",
        "\n",
        "    return np.stack(binned_spectra)  # Shape: (batch_size, num_bins)\n",
        "\n",
        "\n",
        "def process_spectra_with_handler(spectra_batch, num_bins=1000, window_size=200.0):\n",
        "    \"\"\"\n",
        "    Processes spectra batch and returns a list of 1D CNN-ready vectors (one per spectrum),\n",
        "    with sliding window normalization applied.\n",
        "    \"\"\"\n",
        "    spectrum_vectors = []\n",
        "    skipped_spectra = 0\n",
        "\n",
        "    for idx, spectrum in enumerate(spectra_batch):\n",
        "        title = spectrum.get(\"title\", f\"unnamed_{idx}\")\n",
        "        mz_values = np.array(spectrum['mz_values'])\n",
        "        intensity_values = np.array(spectrum['intensity_values'])\n",
        "\n",
        "        if mz_values.size == 0 or intensity_values.size == 0:\n",
        "            print(f\"[SKIPPED] Empty m/z or intensity array: '{title}'\")\n",
        "            skipped_spectra += 1\n",
        "            continue\n",
        "\n",
        "        # Call the binning function with windowed normalization\n",
        "        binned_spectrum = bin_spectra_to_dense_vectors([spectrum], num_bins=num_bins, window_size=window_size)\n",
        "\n",
        "        # Ensure only valid (non-zero) spectra are added\n",
        "        if np.sum(binned_spectrum) == 0:\n",
        "            print(f\"[SKIPPED] Zero intensity after binning: '{title}'\")\n",
        "            skipped_spectra += 1\n",
        "            continue\n",
        "\n",
        "        spectrum_vectors.append(binned_spectrum[0])  # Extract the vector\n",
        "\n",
        "    print(f\"Total skipped spectra: {skipped_spectra}\")\n",
        "    return spectrum_vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-vvcP8E9Fby"
      },
      "source": [
        "## üî¨ Normalize Parent Ion Mass (PEPMASS)\n",
        "\n",
        "This module provides utilities to **extract sequences**, **convert observed m/z to monoisotopic neutral mass** (if needed), and **normalize parent ion values** into the range [0, 1].\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objectives (current implementation)\n",
        "\n",
        "- **Extract** peptide sequence from the beginning of the `TITLE` field.  \n",
        "- **Convert** PEPMASS from **observed m/z** to **monoisotopic single charged mass** when `assume_observed=True`.  \n",
        "- **Normalize** the parent ion mass into \\[0, 1\\] using global bounds from `min_max_dict`.\n",
        "\n",
        "\n",
        "\n",
        "### üß© Key Functions\n",
        "\n",
        "#### üîπ `extract_sequence_from_title(title: str) -> str`\n",
        "Extracts the peptide sequence from the `TITLE`.  \n",
        "Assumes the sequence is the **first token** (before the first space).\n",
        "\n",
        "**Example**\n",
        "```python\n",
        "TITLE = \"GWSMSEQSEESVGGR 2,S,Phospho\"\n",
        "extract_sequence_from_title(TITLE)\n",
        "# ‚Üí \"GWSMSEQSEESVGGR\"\n",
        "```\n",
        "\n",
        "üîπ `observed_to_monoisotopic(observed_mz: float, charge: int) -> float`\n",
        "\n",
        "Converts observed precursor **m/z** into **monoisotopic neutral mass**:\n",
        "\n",
        "$$\n",
        "\\text{mono\\_mass} = z \\cdot \\text{m/z} - (z - 1)\\cdot \\text{PROTON\\_MASS}\n",
        "$$\n",
        "\n",
        "Uses `PROTON_MASS = 1.007276`.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîπ `normalize_parent_ions(data, min_max_dict, assume_observed=True) -> list[float]`\n",
        "\n",
        "Normalizes parent ion values to the range \\$0, 1\\$.\n",
        "\n",
        "* **Inputs per spectrum (dict):**\n",
        "\n",
        "  * `\"pepmass\"`: precursor value\n",
        "  * `\"charge\"`: integer charge state\n",
        "\n",
        "* **Behavior:**\n",
        "\n",
        "  1. If `assume_observed=True`:\n",
        "\n",
        "     * Converts `\"pepmass\"` (observed m/z) ‚Üí monoisotopic neutral mass.\n",
        "  2. If `assume_observed=False`:\n",
        "\n",
        "     * Uses `\"pepmass\"` directly (assumed monoisotopic).\n",
        "  3. Normalizes with:\n",
        "\n",
        "     $$\n",
        "     \\text{norm} = \\frac{parent\\_ion - min}{max - min}\n",
        "     $$\n",
        "  4. Clamps results into \\$0, 1\\$.\n",
        "  5. Missing metadata ‚Üí returns `0.0`.\n",
        "\n",
        "**Example**\n",
        "\n",
        "```python\n",
        "min_max = {\"min\": 500.00, \"max\": 6000.00}\n",
        "normalized = normalize_parent_ions(spectra, min_max, assume_observed=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Output\n",
        "\n",
        "Returns:\n",
        "\n",
        "```python\n",
        "[List of float values between 0 and 1]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Notes\n",
        "\n",
        "* Requires `\"min\"` and `\"max\"` keys in `min_max_dict`.\n",
        "* Missing or invalid metadata defaults to **0.0**.\n",
        "* No theoretical mass calculation or spectrum validation is performed here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_1S6O1fbcC_G"
      },
      "outputs": [],
      "source": [
        "PROTON_MASS = 1.0072764665789\n",
        "H2O_MASS = 18.01056\n",
        "\n",
        "def extract_sequence_from_title(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the peptide sequence from the TITLE string.\n",
        "    Assumes the sequence is the first word, before the first space.\n",
        "    \"\"\"\n",
        "    if not isinstance(title, str) or not title.strip():\n",
        "        return \"\"\n",
        "    return title.strip().split(\" \")[0]  # safe even with extra spaces\n",
        "\n",
        "\n",
        "\n",
        "def observed_to_monoisotopic(observed_mz, charge):\n",
        "    return charge * observed_mz - (charge - 1) * PROTON_MASS\n",
        "\n",
        "\n",
        "\n",
        "def normalize_parent_ions(data, min_max_dict, assume_observed=True):\n",
        "    \"\"\"\n",
        "    Normalize parent ions to the range [0, 1].\n",
        "\n",
        "    If assume_observed=True, converts PEPMASS (observed m/z) to monoisotopic mass before computing normalization.\n",
        "    \"\"\"\n",
        "    normalized = []\n",
        "\n",
        "    for spectrum in data:\n",
        "        pepmass = spectrum.get(\"pepmass\", None)\n",
        "        charge = spectrum.get(\"charge\", None)\n",
        "\n",
        "        if pepmass is None or charge is None:\n",
        "            normalized.append(0.0)\n",
        "            continue\n",
        "\n",
        "        if assume_observed:\n",
        "            mono_mass = observed_to_monoisotopic(pepmass, charge)\n",
        "            parent_ion = mono_mass\n",
        "        else:\n",
        "            parent_ion = pepmass  # Already monoisotopic\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        pepmass_min = min_max_dict[\"min\"]\n",
        "        pepmass_max = min_max_dict[\"max\"]\n",
        "        norm = (parent_ion - pepmass_min) / (pepmass_max - pepmass_min)\n",
        "        normalized.append(max(0, min(1, norm)))\n",
        "\n",
        "    return normalized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUPfkTeX-rQZ"
      },
      "source": [
        "### üß¨ Combine Spectra with Parent Ion Mass, change the model to always recieve monoistopic mass inetas of obserd mass like we currently do.\n",
        "\n",
        "\n",
        "This function constructs the final **input representation** for the neural network by pairing each processed spectrum with its corresponding normalized parent ion mass.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è `combine_features(...)`\n",
        "\n",
        "#### **Purpose**\n",
        "Aggregates spectral and precursor metadata into a unified format, ready to be passed into the model during training or evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Process Flow\n",
        "\n",
        "1. **Spectral Preprocessing**\n",
        "   - Calls `process_spectra_with_handler(...)` to:\n",
        "     - Apply binning and normalization.\n",
        "     - Generate a dense, fixed-length vector for each spectrum.\n",
        "   - Result: `spectra_vectors` ‚Äî a list of shape `[batch_size, num_bins]`.\n",
        "\n",
        "2. **Parent Ion Normalization**\n",
        "   - Invokes `normalize_parent_ions(...)` to:\n",
        "     - Convert precursor monoisotopic mass to observed mass.\n",
        "     - Normalize to a range of [0, 1] using dataset-specific bounds.\n",
        "   - Result: `parent_ions` ‚Äî a list of length `[batch_size]`.\n",
        "\n",
        "3. **Validation**\n",
        "   - Verifies alignment between spectrum vectors and parent ion list.\n",
        "   - Logs an error and aborts if lengths mismatch.\n",
        "\n",
        "4. **Zipping**\n",
        "   - Combines each spectrum vector and its corresponding normalized parent ion into a tuple:\n",
        "     ```python\n",
        "     (spectrum_vector, normalized_parent_ion)\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### üì§ Output Format\n",
        "\n",
        "```python\n",
        "[\n",
        "  (spectrum_vec‚ÇÅ, pepmass‚ÇÅ),\n",
        "  (spectrum_vec‚ÇÇ, pepmass‚ÇÇ),\n",
        "  ...\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-4v207cecaPq"
      },
      "outputs": [],
      "source": [
        "def combine_features(data, pepmass_min_max, num_bins, window_normaliation_size, assume_observed):\n",
        "    \"\"\"\n",
        "    Converts spectra + metadata into model input tuples:\n",
        "        (binned spectrum, normalized parent ion mass)\n",
        "    \"\"\"\n",
        "\n",
        "    spectra_vectors = process_spectra_with_handler(data, num_bins, window_normaliation_size)\n",
        "    if not spectra_vectors:\n",
        "        return None\n",
        "\n",
        "    parent_ions = parent_ions = normalize_parent_ions(\n",
        "    data, pepmass_min_max, assume_observed=assume_observed)\n",
        "\n",
        "\n",
        "    if len(spectra_vectors) != len(parent_ions):\n",
        "        print(\"‚ùå Mismatch between spectra and parent ions.\")\n",
        "        return None\n",
        "\n",
        "    return list(zip(spectra_vectors, parent_ions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0vi2fg2-83n"
      },
      "source": [
        "### üè∑Ô∏è Label Spectra Based on Modifications\n",
        "\n",
        "This function performs **automatic labeling** of MS/MS spectra for supervised learning, based on the content of the `TITLE` field in each spectrum's metadata.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Purpose\n",
        "\n",
        "Assigns integer labels to each spectrum in a batch according to the presence of post-translational modification (PTM) keywords in the title:\n",
        "\n",
        "- `0` ‚Üí **Unmodified**\n",
        "- `1` ‚Üí **Oxidation** (if the word `\"oxidation\"` appears in the title)\n",
        "- `2` ‚Üí **Phosphorylation** (if the word `\"phospho\"` appears in the title)\n",
        "\n",
        "The result is a list of labels aligned with the order of input spectra ‚Äî suitable for classification tasks using `CrossEntropyLoss`, `BCEWithLogitsLoss`, or one-hot encoding strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Logic\n",
        "\n",
        "For each spectrum in the input list:\n",
        "1. Checks that the entry is a dictionary.\n",
        "2. Extracts the `title` and converts it to lowercase.\n",
        "3. Searches for PTM-related keywords.\n",
        "4. Defaults to `0` if no match or invalid format.\n",
        "\n",
        "---\n",
        "\n",
        "### üì§ Output Format\n",
        "\n",
        "Returns:\n",
        "```python\n",
        "[0, 2, 1, 0, 1, ...]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jyBnMMjYeF6j"
      },
      "outputs": [],
      "source": [
        "#This cell reads the labels of the data and prepares it for the model\n",
        "\n",
        "def spectrum_label(spectra_data) -> list:\n",
        "    \"\"\"\n",
        "    Assigns labels to spectra based on known modifications in TITLE.\n",
        "\n",
        "    Parameters:\n",
        "    - spectra_data (list of dict): List of spectrum dictionaries (from DatasetHandler).\n",
        "\n",
        "    Returns:\n",
        "    - List of labels for each spectrum.\n",
        "    \"\"\"\n",
        "    if not isinstance(spectra_data, list):\n",
        "        print(\"ERROR: Expected a list of spectra, got\", type(spectra_data))\n",
        "        return None\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    for spectrum in spectra_data:\n",
        "        if not isinstance(spectrum, dict):\n",
        "            print(f\"WARNING: Expected spectrum to be a dict, got {type(spectrum)}\")\n",
        "            labels.append(0)\n",
        "            continue\n",
        "\n",
        "        # Get spectrum title and verify it's a non-empty string\n",
        "        spectrum_id = spectrum.get(\"title\", \"\")\n",
        "        if not isinstance(spectrum_id, str) or not spectrum_id.strip():\n",
        "            print(f\"WARNING: Missing or invalid title for spectrum, assigning label 0\")\n",
        "            labels.append(0)\n",
        "            continue\n",
        "\n",
        "        spectrum_id = spectrum_id.lower().strip()  # Normalize for label detection\n",
        "\n",
        "        # Assign labels based on keywords in TITLE\n",
        "        if \"oxidation\" in spectrum_id:\n",
        "            labels.append(1)\n",
        "        elif \"phospho\" in spectrum_id:\n",
        "            labels.append(2)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "\n",
        "    print(f\"Labels: {Counter(labels)}\")\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnQJzcNm_FmC"
      },
      "source": [
        "---\n",
        "\n",
        "# üß† Hybrid CNN-Transformer Multi-Label Classifier\n",
        "\n",
        "This module defines the **final architecture** used for **multi-label PTM classification** from MS/MS spectra.\n",
        "The model integrates **local pattern extraction (CNN)**, **global context modeling (Transformer)**, and **metadata (parent ion mass)**, and predicts each class with an **independent MLP head** (one-vs-rest).\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ `PositionalEncoding`\n",
        "\n",
        "Implements **sinusoidal positional encodings** (Vaswani et al., 2017), injecting sequence order information into embeddings.\n",
        "\n",
        "* **Signature:**\n",
        "\n",
        "  ```python\n",
        "  PositionalEncoding(d_model: int = 64, seq_len: int = 4500, dropout: float = 0.1)\n",
        "  ```\n",
        "* **Behavior:** Precomputes a tensor of `sin`/`cos` terms and adds it to the input, followed by dropout.\n",
        "* **Input:** `[B, L, d_model]` with `L ‚â§ seq_len`\n",
        "* **Output:** Same shape as input\n",
        "\n",
        "**Example**\n",
        "\n",
        "```python\n",
        "pe = PositionalEncoding()\n",
        "x = torch.randn(32, 10, 64)   # [batch, seq_len, d_model]\n",
        "x = pe(x)                     # same shape\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ `MLPHead`\n",
        "\n",
        "A compact one-vs-rest head producing a **single logit** per class.\n",
        "\n",
        "```python\n",
        "MLPHead(input_dim, hidden_dim=64, dropout=0.5)\n",
        "# Linear(input_dim ‚Üí hidden_dim) ‚Üí ReLU ‚Üí Dropout ‚Üí Linear(hidden_dim ‚Üí 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ `EncoderTransformerClassifier`\n",
        "\n",
        "A **hybrid classifier** with five main blocks:\n",
        "\n",
        "1. **1D CNN Encoder** ‚Äì Extracts local spectral patterns\n",
        "\n",
        "   ```\n",
        "   Conv1d(1‚Üí32, k=5, pad=2) ‚Üí BN ‚Üí ReLU\n",
        "   MaxPool1d(k=2)            # halves length\n",
        "   Conv1d(32‚Üí64, k=3, pad=1) ‚Üí BN ‚Üí ReLU\n",
        "   Flatten\n",
        "   ```\n",
        "\n",
        "   * **Output:** `[B, 64 * (input_size // 2)]`\n",
        "\n",
        "2. **Linear Encoder** ‚Äì Projects CNN features into Transformer latent space\n",
        "\n",
        "   ```\n",
        "   Linear(64*(S/2) ‚Üí 512) ‚Üí BN ‚Üí ReLU\n",
        "   Linear(512 ‚Üí latent_size) ‚Üí BN ‚Üí ReLU ‚Üí Dropout\n",
        "   ```\n",
        "\n",
        "   * **Output:** `[B, latent_size]`\n",
        "\n",
        "3. **Positional Encoding + Transformer** ‚Äì Global context\n",
        "\n",
        "   * Expand to sequence: `[B, 1, latent_size]`\n",
        "   * Add sinusoidal encoding\n",
        "   * `nn.TransformerEncoder` with:\n",
        "\n",
        "     * `num_layers`, `num_heads`\n",
        "     * `dim_feedforward = 4 * latent_size`\n",
        "     * `dropout = dropout_prob`\n",
        "     * `batch_first=True`, **`norm_first=True`**\n",
        "   * Mean over sequence dim ‚Üí `[B, latent_size]`\n",
        "\n",
        "4. **Parent Ion Processor** ‚Äì Encodes normalized parent mass\n",
        "\n",
        "   ```\n",
        "   Linear(1 ‚Üí 64) ‚Üí ReLU\n",
        "   Linear(64 ‚Üí latent_size) ‚Üí ReLU\n",
        "   ```\n",
        "\n",
        "   * **Output:** `[B, latent_size]`\n",
        "\n",
        "5. **Fusion & One-vs-Rest Heads**\n",
        "\n",
        "   ```\n",
        "   concat([spectrum, parent]) ‚Üí [B, 2*latent_size]\n",
        "   Dropout\n",
        "   Heads: 3 √ó MLPHead(2*latent_size ‚Üí 1 logit)\n",
        "   ```\n",
        "\n",
        "   * **Output:** logits `[B, 3]` (concatenated from three heads)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Forward Pass\n",
        "\n",
        "**Inputs**\n",
        "\n",
        "* `spectra`: `[B, S]` (dense binned spectrum, length = `input_size`)\n",
        "* `parent_ion`: `[B]` (normalized precursor mass)\n",
        "\n",
        "**Output**\n",
        "\n",
        "* `logits`: `[B, 3]` ‚Äî **independent logits per class** (multi-label)\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Implementation Notes\n",
        "\n",
        "* `latent_size % num_heads == 0` is enforced.\n",
        "* `input_size` must be **even** (due to `MaxPool1d`).\n",
        "* The Transformer currently sees only **one token** per spectrum (a global embedding).\n",
        "  To enable attention over multiple tokens, feed a sequence (e.g., retain the CNN temporal dimension before flattening).\n",
        "* The current implementation instantiates **3 heads** explicitly:\n",
        "\n",
        "  ```python\n",
        "  self.heads = nn.ModuleList([MLPHead(2*latent_size, hidden_dim=latent_size, dropout=dropout_prob) for _ in range(3)])\n",
        "  ```\n",
        "\n",
        "  If you want it to follow `num_classes`, change `range(3)` to `range(num_classes)`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example\n",
        "\n",
        "```python\n",
        "model = EncoderTransformerClassifier(\n",
        "    input_size=175, latent_size=64, num_classes=3,   # num_classes stored; heads currently fixed to 3\n",
        "    num_heads=4, num_layers=2, dropout_prob=0.1\n",
        ")\n",
        "\n",
        "spectra = torch.randn(32, 175)  # [batch, S]\n",
        "parent  = torch.rand(32)        # [batch], normalized\n",
        "logits  = model((spectra, parent))  # [32, 3]\n",
        "```\n",
        "\n",
        "**Loss (Multi-label)**\n",
        "\n",
        "Use **independent** sigmoid + **BCEWithLogitsLoss** with multi-hot targets of shape `[B, 3]`:\n",
        "\n",
        "```python\n",
        "targets = torch.tensor([[1,0,1], [0,1,0], ...], dtype=torch.float32)  # multi-hot per sample\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "loss = loss_fn(logits, targets)\n",
        "```\n",
        "\n",
        "**Inference (per-class probabilities & thresholds)**\n",
        "\n",
        "```python\n",
        "probs = torch.sigmoid(logits)       # [B, 3]\n",
        "preds = (probs >= 0.5).int()        # thresholdable per class\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eMdLs4YZeHba"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int = 64, seq_len: int = 4500, dropout: float = 0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, seq_len, d_model]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MLPHead(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class EncoderTransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_size, latent_size, num_classes, num_heads, num_layers, dropout_prob, max_len=1000):\n",
        "        super(EncoderTransformerClassifier, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Validate divisibility\n",
        "        if latent_size % num_heads != 0:\n",
        "            raise ValueError(f\"latent_size ({latent_size}) must be divisible by num_heads ({num_heads}).\")\n",
        "\n",
        "        # 1. CNN Encoder (New Layer)\n",
        "        self.cnn_encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),  # Downsample\n",
        "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # 2. Linear Encoder (Refactored)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(64 * (input_size // 2), 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, latent_size),\n",
        "            nn.BatchNorm1d(latent_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        #3. Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model=latent_size, seq_len=max_len, dropout=dropout_prob)\n",
        "\n",
        "        # 4. Transformer Encoder\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=latent_size,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=latent_size * 4,\n",
        "                dropout=dropout_prob,\n",
        "                activation='relu',\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Parent Ion Layer\n",
        "        self.parent_ion_layer = nn.Sequential(\n",
        "            nn.Linear(1, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.latent_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Dropout before classification\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # One-vs-Rest heads (multi-label)\n",
        "        self.heads = nn.ModuleList([\n",
        "            MLPHead(latent_size * 2, hidden_dim=latent_size, dropout=dropout_prob) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        spectra, parent_ion = inputs\n",
        "        parent_ion = parent_ion.unsqueeze(1)\n",
        "\n",
        "        # CNN Encoder\n",
        "        spectra = spectra.unsqueeze(1)  # Ensure input is [B, 1, S]\n",
        "        cnn_output = self.cnn_encoder(spectra)\n",
        "\n",
        "        # Linear Encoder\n",
        "        x = self.encoder(cnn_output)\n",
        "\n",
        "        # Positional Encoding and Transformer\n",
        "        x = x.unsqueeze(1)  # Adding sequence dimension\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Parent Ion Encoding\n",
        "        parent = self.parent_ion_layer(parent_ion).squeeze(1)\n",
        "\n",
        "        # Concatenate\n",
        "        combined = torch.cat([x, parent], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        # MLP Heads\n",
        "        outputs = [head(combined) for head in self.heads]\n",
        "        return torch.cat(outputs, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtvKBR0K_RsL"
      },
      "source": [
        "### üß™ Training, Evaluation & Logging Utilities\n",
        "\n",
        "This section defines core utility functions used to train, evaluate, and monitor the Transformer-based classifier on MS/MS spectra.\n",
        "\n",
        "---\n",
        "\n",
        "#### üóÇÔ∏è Logging Setup\n",
        "Configures a logging pipeline that:\n",
        "- Removes any pre-existing Colab logging handlers to avoid duplication.\n",
        "- Logs model performance, evaluation scores, and batch-level information to a persistent file on Google Drive.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ `train_classifier_with_weights(...)`\n",
        "\n",
        "Trains the Transformer classifier using **weighted binary cross-entropy loss** to handle **class imbalance**. Key features:\n",
        "- Dynamically computes class weights based on training batch distribution.\n",
        "- Incorporates **L1 regularization** to prevent overfitting and encourage sparsity.\n",
        "- Logs epoch-wise loss and accuracy.\n",
        "- Saves model weights to disk upon completion.\n",
        "\n",
        "Uses **one-hot encoded labels** and tracks prediction accuracy by comparing rounded sigmoid outputs to targets.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä `evaluate_model(...)`\n",
        "\n",
        "Evaluates the model on a validation batch with the following features:\n",
        "- Converts integer labels to multi-hot vectors.\n",
        "- Applies sigmoid activation to get predicted class probabilities.\n",
        "- Computes:\n",
        "  - **Macro** and **Weighted**: Precision, Recall, F1-score\n",
        "  - **ROC-AUC** and **PR-AUC**\n",
        "  - Class distribution and per-class metrics via `classification_report`\n",
        "- Computes a custom **composite score** weighted toward macro F1, PR-AUC, and recall to better reflect performance under class imbalance.\n",
        "\n",
        "### üìä Custom Evaluation Score\n",
        "\n",
        "To reflect the model's performance priorities (especially under class imbalance), we define a **composite evaluation score** as a weighted sum of key metrics:\n",
        "\n",
        "$$\n",
        "\\text{Score} = 0.40 \\cdot \\text{Macro-F1} + 0.35 \\cdot \\text{PR-AUC} + 0.25 \\cdot \\text{Macro-Recall} + 0.05 \\cdot \\text{ROC-AUC}\n",
        "$$\n",
        "---\n",
        "\n",
        "### üß† Rationale\n",
        "\n",
        "- **Macro-F1 (40%)**: Emphasizes balanced per-class precision and recall.\n",
        "- **PR-AUC (35%)**: Prioritizes performance on rare classes (especially PTMs).\n",
        "- **Macro-Recall (25%)**: Promotes sensitivity (true positive rate) across all classes.\n",
        "- **ROC-AUC (5%)**: Included for completeness, but de-emphasized due to its limitations under class imbalance.\n",
        "\n",
        "This scoring approach guides model selection and checkpointing toward better performance on rare and biologically meaningful PTMs like oxidation and phosphorylation.\n",
        "\n",
        "---\n",
        "\n",
        "These utilities provide a robust training pipeline with integrated evaluation, threshold tuning (optional), and systematic logging for reproducibility and model comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QVIE38Jwesqv"
      },
      "outputs": [],
      "source": [
        "#Testing, evaluating and logging cell\n",
        "\n",
        "#Remove existing handlers to prevent Colab caching issues\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "log_dir = \"/content/drive/MyDrive/peak_encoder_transformer_pipeline/logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "log_path = os.path.join(log_dir, \"binning_transformer.log\")\n",
        "\n",
        "# Reapply config AFTER handlers are cleared\n",
        "logging.basicConfig(\n",
        "    filename=log_path,\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    filemode=\"a\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"spectra_logger\")\n",
        "\n",
        "\n",
        "\n",
        "def train_classifier_with_weights(model, data_tensors, labels, epochs=100, learning_rate=0.001,l1_lambda=0.001, save_path=\"model_final_weights.pth\", device='cuda'):\n",
        "    \"\"\"\n",
        "    Trains the encoder-classifier model with a weighted loss function for handling class imbalance.\n",
        "    Assumes all inputs are already PyTorch tensors on the correct device.\n",
        "    \"\"\"\n",
        "    spectra_tensors, parent_ions = data_tensors\n",
        "    parent_ions = parent_ions.unsqueeze(1)  # Ensure shape (batch_size, 1)\n",
        "    labels_tensors = labels  # Already a tensor\n",
        "\n",
        "    # Compute class weights (just once on CPU for Counter)\n",
        "    class_counts = Counter(labels_tensors.cpu().numpy())\n",
        "    num_classes = torch.max(labels_tensors).item() + 1\n",
        "    class_weights = torch.tensor([1.0, 5.0, 2.0], dtype=torch.float32).to(device)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        if cls in class_counts:\n",
        "            class_weights[cls] = len(labels_tensors) / (num_classes * class_counts[cls])\n",
        "\n",
        "    print(f\"Class Weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Optimizer & Loss\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01) #SGD worth a try?\n",
        "    loss_fn = nn.BCEWithLogitsLoss()  # or add pos_weight=torch.tensor([...]) if it performs badly\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model((spectra_tensors ,parent_ions))\n",
        "        labels_onehot = F.one_hot(labels_tensors, num_classes=3).float()\n",
        "        loss = loss_fn(outputs, labels_onehot)\n",
        "\n",
        "        # L1 Regularization\n",
        "        l1_loss = 0.0\n",
        "        for param in model.parameters():\n",
        "            l1_loss += torch.sum(torch.abs(param))\n",
        "        loss += l1_lambda * l1_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        probs = torch.sigmoid(outputs)  # (batch_size, 3)\n",
        "        predictions = (probs >= 0.5).float()\n",
        "        accuracy = ((predictions == labels_onehot).float().mean()).item()\n",
        "\n",
        "\n",
        "        #Store metrics\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accuracies.append(accuracy)\n",
        "\n",
        "        #Log epoch details\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Debugging Information\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            print(f\"Sample Predictions: {predictions[:5].cpu().numpy()}\")\n",
        "            print(f\"Actual Labels: {labels[:5]}\")\n",
        "            print(f\"Sample Logits: {outputs[:5].detach().cpu().numpy()}\")\n",
        "\n",
        "    #Save model weights after training\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Final model weights saved to {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_tensors, labels, batch=None):\n",
        "    \"\"\"\n",
        "    Evaluates a One-vs-Rest (multi-label) classifier and computes performance metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    spectra_tensors, parent_ions = data_tensors\n",
        "    parent_ions = parent_ions.unsqueeze(1)\n",
        "\n",
        "    # Convert integer labels (0/1/2) to multi-hot vectors [batch_size, 3]\n",
        "    if labels.ndim == 1:  # integer labels\n",
        "        labels = F.one_hot(labels, num_classes=3).float()\n",
        "    targets = labels  # multi-hot tensor\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model((spectra_tensors, parent_ions))\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Get sigmoid probabilities\n",
        "        probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
        "        predictions = (probabilities >= 0.5).astype(int)\n",
        "        targets_np = targets.cpu().numpy()\n",
        "\n",
        "        # Macro & Weighted scores\n",
        "        macro_precision = precision_score(targets_np, predictions, average='macro', zero_division=0)\n",
        "        macro_recall = recall_score(targets_np, predictions, average='macro', zero_division=0)\n",
        "        macro_f1 = f1_score(targets_np, predictions, average='macro', zero_division=0)\n",
        "        weighted_precision = precision_score(targets_np, predictions, average='weighted', zero_division=0)\n",
        "        weighted_recall = recall_score(targets_np, predictions, average='weighted', zero_division=0)\n",
        "        weighted_f1 = f1_score(targets_np, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        # ROC & PR AUC\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(targets_np, probabilities, average='macro')\n",
        "        except ValueError:\n",
        "            roc_auc = float('nan')\n",
        "        try:\n",
        "            pr_auc = average_precision_score(targets_np, probabilities, average=\"macro\")\n",
        "        except ValueError:\n",
        "            pr_auc = float('nan')\n",
        "\n",
        "        # Class distribution\n",
        "        class_distribution = Counter(np.argmax(targets_np, axis=1))\n",
        "\n",
        "        # Per-class report\n",
        "        class_names = [\"Unmodified\", \"Oxidation\", \"Phospho\"]\n",
        "        report = classification_report(targets_np, predictions, target_names=class_names, zero_division=0, digits=4)\n",
        "\n",
        "        # Weighing multiple metrics to reflect your priorities\n",
        "        score = (\n",
        "        0.40 * macro_f1 +    # overall per-class balance (adjusted slightly lower)\n",
        "        0.35 * pr_auc +      # good for rare classes (increased weight for rare mods)\n",
        "        0.25 * macro_recall +# favors finding mods (increased recall emphasis)\n",
        "        0.05 * roc_auc       # optional, less informative in imbalance (reduced further)\n",
        "    )\n",
        "\n",
        "        log_message = (\n",
        "            f\"Batch {batch if batch is not None else '-'}: Validation Loss: {loss.item():.4f},\\n\"\n",
        "            f\"Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}, Macro F1-score: {macro_f1:.4f},\\n\"\n",
        "            f\"Weighted Precision: {weighted_precision:.4f}, Weighted Recall: {weighted_recall:.4f}, Weighted F1-score: {weighted_f1:.4f},\\n\"\n",
        "            f\"ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\\n\"\n",
        "            f\"Class Distribution: {class_distribution}\\n\"\n",
        "            f\"Per-class metrics:\\n{report}\"\n",
        "        )\n",
        "\n",
        "        print(log_message)\n",
        "        logger.info(log_message)\n",
        "\n",
        "    return  score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OQaeRCr_aLI"
      },
      "source": [
        "### üöÄ Model Training Loop with Batching and On-the-Fly Data Loading\n",
        "\n",
        "This section implements the full training pipeline, designed for efficient, memory-conscious processing of `.mgf` spectral files in batches. It ensures dynamic model improvement and continuous evaluation throughout the training process.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Hyperparameters and Configuration\n",
        "\n",
        "The training loop is governed by a structured set of parameters, categorized by their role and optimization status. Below is a breakdown of all parameters used in the pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìà Tuned Hyperparameters (via Optuna)\n",
        "\n",
        "| Hyperparameter  | Description                                                            |\n",
        "| --------------- | ---------------------------------------------------------------------- |\n",
        "| `latent_size`   | Dimensionality of the latent space (`{64, 128, 256}`)                  |\n",
        "| `dropout_prob`  | Dropout rate to prevent overfitting (`[0.1, 0.5]`)                     |\n",
        "| `learning_rate` | Learning rate for the `AdamW` optimizer (`[1e-4, 1e-2]`, log-scaled)   |\n",
        "| `num_heads`     | Number of attention heads in the Transformer (`{2, 4, 8}`)             |\n",
        "| `num_layers`    | Number of Transformer encoder layers (`[2, 6]`)                        |\n",
        "| `l1_lambda`     | L1 regularization strength (`[1e-7, 1e-4]`, log-scaled)                |\n",
        "| `num_bins`      | Number of m/z bins for input discretization (`[500, 10000]`, step=500) |\n",
        "\n",
        "---\n",
        "\n",
        "#### üß± Fixed Architectural Parameters\n",
        "\n",
        "| Parameter     | Description                                                       |\n",
        "| ------------- | ----------------------------------------------------------------- |\n",
        "| `num_classes` | Number of output classes (unmodified, oxidation, phosphorylation) |\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Preprocessing Configuration (Fixed)\n",
        "\n",
        "| Parameter                  | Description                                                               |\n",
        "| -------------------------- | ------------------------------------------------------------------------- |\n",
        "| `window_normaliation_size` | m/z window size for sliding normalization (e.g., 200.0)                   |\n",
        "| `pepmass_range`            | Normalization bounds for parent ion mass: `{'min': 500.00, 'max': 6000.0}` |\n",
        "\n",
        "---\n",
        "\n",
        "#### üñ•Ô∏è Runtime & Training Control\n",
        "\n",
        "| Parameter                         | Description                                                          |\n",
        "| --------------------------------- | -------------------------------------------------------------------- |\n",
        "| `epoch`                           | Number of training epochs per mini-batch (typically 100)             |\n",
        "| `num_loops`                       | Number of full passes over the dataset via `DatasetHandler`          |\n",
        "| `device`                          | GPU or CPU (automatically detected with `torch.cuda.is_available()`) |\n",
        "| `min_score_threshold`             | Minimum evaluation score to trigger model checkpoint saving          |\n",
        "| `input_dir` / `model_weights_dir` | File paths for dataset and weight storage                            |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### üß± Model and Dataset Setup\n",
        "\n",
        "- Initializes the `EncoderTransformerClassifier` with the specified architecture and moves it to GPU.\n",
        "- Uses `DatasetHandler` to iterate through the dataset stored in `.mgf` format, supporting multiple full passes (controlled by `num_loops`).\n",
        "\n",
        "---\n",
        "\n",
        "#### üîÅ Training Loop\n",
        "\n",
        "Each iteration of the loop processes one `.mgf` file as a mini-batch:\n",
        "\n",
        "1. **Model Reloading**: Attempts to load the most recent model weights (`latest_model.pth`) before training a new batch.\n",
        "2. **Feature Engineering**:\n",
        "   - Converts spectra to binned vectors and normalizes precursor masses.\n",
        "   - Packs features into tensors ready for the model.\n",
        "3. **Label Assignment**: Reads spectrum titles to determine PTM class (label 0‚Äì2).\n",
        "4. **Train/Validation Split**: Ensures stratified separation for fair evaluation (80/20).\n",
        "5. **Training**: Trains the model for the specified number of epochs on the batch using `train_classifier_with_weights(...)`.\n",
        "6. **Evaluation**:\n",
        "   - Evaluates the model on the validation split using `evaluate_model(...)`.\n",
        "   - Computes a custom performance score which envisions maximize the difereantiation between clasees.\n",
        "   - Compares against the best score so far to decide whether to checkpoint the model.\n",
        "\n",
        "---\n",
        "\n",
        "#### üíæ Model Checkpointing\n",
        "\n",
        "If the evaluation score improves and the batch is not excluded (e.g., `split_file_251.mgf` due to beign a smaller not represntative file in our specific dataset), the model is saved with a timestamp and score in the filename for version tracking.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Training Termination\n",
        "\n",
        "The loop continues until all files have been processed. No `break`-based interruption is needed manually ‚Äî the `DatasetHandler` handles loop exits cleanly.\n",
        "\n",
        "---\n",
        "\n",
        "This design allows for **scalable**, **robust**, and **traceable training** across many MGF batches without requiring all data in memory at once.\n",
        "\n",
        "#### üíæ Model Checkpointing\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Training Termination\n",
        "\n",
        "The loop continues until all files have been processed. No `break`-based interruption is needed manually ‚Äî the `DatasetHandler` handles loop exits cleanly.\n",
        "\n",
        "---\n",
        "\n",
        "This design allows for **scalable**, **robust**, and **traceable training** across many MGF batches without requiring all data in memory at once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FNFHDUb0exkv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1c09b4a-113a-4258-9a97-d71b0e955bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model before starting the training loop.\n",
            "Processing file: /content/drive/MyDrive/data/mini3/split_file_001.mgf\n",
            "Loaded model before training batch 1.\n",
            "Total skipped spectra: 0\n",
            "Labels: Counter({0: 401, 1: 400, 2: 400})\n",
            "Class Weights: [1. 1. 1.]\n",
            "Epoch [1/100] - Loss: 0.6590, Accuracy: 83.82%\n",
            "Epoch [2/100] - Loss: 0.3080, Accuracy: 91.81%\n",
            "Epoch [3/100] - Loss: 0.1865, Accuracy: 95.10%\n",
            "Epoch [4/100] - Loss: 0.1297, Accuracy: 96.84%\n",
            "Epoch [5/100] - Loss: 0.0955, Accuracy: 97.92%\n",
            "Epoch [6/100] - Loss: 0.0747, Accuracy: 98.72%\n",
            "Epoch [7/100] - Loss: 0.0660, Accuracy: 98.89%\n",
            "Epoch [8/100] - Loss: 0.0613, Accuracy: 99.27%\n",
            "Epoch [9/100] - Loss: 0.0524, Accuracy: 99.44%\n",
            "Epoch [10/100] - Loss: 0.0513, Accuracy: 99.55%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[-8.064365   7.371846  -9.737116 ]\n",
            " [-7.0836554 -8.339902   8.03067  ]\n",
            " [ 5.1897936 -6.569386  -8.740329 ]\n",
            " [ 3.7311103 -4.434356  -5.851869 ]\n",
            " [-4.5788975 -4.248362   3.5728855]]\n",
            "Epoch [11/100] - Loss: 0.0501, Accuracy: 99.65%\n",
            "Epoch [12/100] - Loss: 0.0472, Accuracy: 99.76%\n",
            "Epoch [13/100] - Loss: 0.0456, Accuracy: 99.69%\n",
            "Epoch [14/100] - Loss: 0.0455, Accuracy: 99.79%\n",
            "Epoch [15/100] - Loss: 0.0436, Accuracy: 99.90%\n",
            "Epoch [16/100] - Loss: 0.0425, Accuracy: 99.93%\n",
            "Epoch [17/100] - Loss: 0.0413, Accuracy: 99.93%\n",
            "Epoch [18/100] - Loss: 0.0412, Accuracy: 99.93%\n",
            "Epoch [19/100] - Loss: 0.0411, Accuracy: 99.97%\n",
            "Epoch [20/100] - Loss: 0.0406, Accuracy: 99.97%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -8.072183    7.6630507  -8.213162 ]\n",
            " [ -5.324483   -6.580817    6.255715 ]\n",
            " [  6.8073754  -5.73918   -10.385643 ]\n",
            " [  6.8867316  -6.727808   -9.16427  ]\n",
            " [ -9.516632   -9.692292    9.013911 ]]\n",
            "Epoch [21/100] - Loss: 0.0404, Accuracy: 100.00%\n",
            "Epoch [22/100] - Loss: 0.0395, Accuracy: 100.00%\n",
            "Epoch [23/100] - Loss: 0.0400, Accuracy: 100.00%\n",
            "Epoch [24/100] - Loss: 0.0394, Accuracy: 99.97%\n",
            "Epoch [25/100] - Loss: 0.0392, Accuracy: 100.00%\n",
            "Epoch [26/100] - Loss: 0.0395, Accuracy: 99.97%\n",
            "Epoch [27/100] - Loss: 0.0389, Accuracy: 100.00%\n",
            "Epoch [28/100] - Loss: 0.0382, Accuracy: 100.00%\n",
            "Epoch [29/100] - Loss: 0.0381, Accuracy: 100.00%\n",
            "Epoch [30/100] - Loss: 0.0382, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -8.287686    5.851046  -12.473725 ]\n",
            " [ -7.4108424  -9.592979    7.0555325]\n",
            " [  4.3590493  -4.947192   -5.2483063]\n",
            " [  6.061068   -5.5515337  -9.879332 ]\n",
            " [ -6.5890474  -8.248535    6.6925797]]\n",
            "Epoch [31/100] - Loss: 0.0379, Accuracy: 100.00%\n",
            "Epoch [32/100] - Loss: 0.0376, Accuracy: 100.00%\n",
            "Epoch [33/100] - Loss: 0.0378, Accuracy: 100.00%\n",
            "Epoch [34/100] - Loss: 0.0378, Accuracy: 100.00%\n",
            "Epoch [35/100] - Loss: 0.0371, Accuracy: 100.00%\n",
            "Epoch [36/100] - Loss: 0.0379, Accuracy: 99.97%\n",
            "Epoch [37/100] - Loss: 0.0369, Accuracy: 100.00%\n",
            "Epoch [38/100] - Loss: 0.0368, Accuracy: 100.00%\n",
            "Epoch [39/100] - Loss: 0.0367, Accuracy: 100.00%\n",
            "Epoch [40/100] - Loss: 0.0378, Accuracy: 99.93%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -7.095309    6.7163444  -9.043212 ]\n",
            " [-10.964428   -9.600055    7.75968  ]\n",
            " [  5.8574367  -6.2900686  -8.217745 ]\n",
            " [  7.5885663  -9.835515  -14.538208 ]\n",
            " [ -8.446679  -10.117342    6.1571717]]\n",
            "Epoch [41/100] - Loss: 0.0370, Accuracy: 99.97%\n",
            "Epoch [42/100] - Loss: 0.0364, Accuracy: 100.00%\n",
            "Epoch [43/100] - Loss: 0.0363, Accuracy: 100.00%\n",
            "Epoch [44/100] - Loss: 0.0360, Accuracy: 100.00%\n",
            "Epoch [45/100] - Loss: 0.0359, Accuracy: 100.00%\n",
            "Epoch [46/100] - Loss: 0.0360, Accuracy: 100.00%\n",
            "Epoch [47/100] - Loss: 0.0358, Accuracy: 100.00%\n",
            "Epoch [48/100] - Loss: 0.0359, Accuracy: 100.00%\n",
            "Epoch [49/100] - Loss: 0.0359, Accuracy: 100.00%\n",
            "Epoch [50/100] - Loss: 0.0357, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -8.102814    6.1957507  -9.834093 ]\n",
            " [ -7.199683   -8.577939    7.0136847]\n",
            " [  5.7165327  -6.2237105  -9.536738 ]\n",
            " [  6.1037965  -6.7659597 -10.836515 ]\n",
            " [-11.406299   -8.873505    8.485878 ]]\n",
            "Epoch [51/100] - Loss: 0.0354, Accuracy: 100.00%\n",
            "Epoch [52/100] - Loss: 0.0353, Accuracy: 100.00%\n",
            "Epoch [53/100] - Loss: 0.0353, Accuracy: 100.00%\n",
            "Epoch [54/100] - Loss: 0.0350, Accuracy: 100.00%\n",
            "Epoch [55/100] - Loss: 0.0351, Accuracy: 100.00%\n",
            "Epoch [56/100] - Loss: 0.0349, Accuracy: 100.00%\n",
            "Epoch [57/100] - Loss: 0.0347, Accuracy: 100.00%\n",
            "Epoch [58/100] - Loss: 0.0349, Accuracy: 100.00%\n",
            "Epoch [59/100] - Loss: 0.0346, Accuracy: 100.00%\n",
            "Epoch [60/100] - Loss: 0.0344, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -8.355305    7.4424286 -10.6488495]\n",
            " [ -8.969304   -7.7046595   6.6844797]\n",
            " [  8.312987   -8.535254  -11.829972 ]\n",
            " [  7.0276566  -7.0088024 -11.345263 ]\n",
            " [-11.745373   -7.2793984   7.2369933]]\n",
            "Epoch [61/100] - Loss: 0.0344, Accuracy: 100.00%\n",
            "Epoch [62/100] - Loss: 0.0342, Accuracy: 100.00%\n",
            "Epoch [63/100] - Loss: 0.0343, Accuracy: 100.00%\n",
            "Epoch [64/100] - Loss: 0.0342, Accuracy: 100.00%\n",
            "Epoch [65/100] - Loss: 0.0340, Accuracy: 100.00%\n",
            "Epoch [66/100] - Loss: 0.0339, Accuracy: 100.00%\n",
            "Epoch [67/100] - Loss: 0.0339, Accuracy: 100.00%\n",
            "Epoch [68/100] - Loss: 0.0340, Accuracy: 100.00%\n",
            "Epoch [69/100] - Loss: 0.0338, Accuracy: 100.00%\n",
            "Epoch [70/100] - Loss: 0.0337, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -9.319691   10.344977  -13.98086  ]\n",
            " [ -9.176564   -8.183281    8.084192 ]\n",
            " [  9.752053  -10.1841955 -11.465153 ]\n",
            " [  4.7653494  -5.645228  -10.474876 ]\n",
            " [-11.225754   -9.428773    7.162865 ]]\n",
            "Epoch [71/100] - Loss: 0.0336, Accuracy: 100.00%\n",
            "Epoch [72/100] - Loss: 0.0334, Accuracy: 100.00%\n",
            "Epoch [73/100] - Loss: 0.0335, Accuracy: 100.00%\n",
            "Epoch [74/100] - Loss: 0.0336, Accuracy: 100.00%\n",
            "Epoch [75/100] - Loss: 0.0334, Accuracy: 100.00%\n",
            "Epoch [76/100] - Loss: 0.0331, Accuracy: 100.00%\n",
            "Epoch [77/100] - Loss: 0.0331, Accuracy: 100.00%\n",
            "Epoch [78/100] - Loss: 0.0329, Accuracy: 100.00%\n",
            "Epoch [79/100] - Loss: 0.0328, Accuracy: 100.00%\n",
            "Epoch [80/100] - Loss: 0.0330, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -9.026434    8.879146  -11.962285 ]\n",
            " [ -8.531796  -11.539696    7.2007613]\n",
            " [  7.500562   -6.321666   -9.510429 ]\n",
            " [  6.988286   -8.131984   -8.640127 ]\n",
            " [-12.5296955 -11.044977   10.203334 ]]\n",
            "Epoch [81/100] - Loss: 0.0327, Accuracy: 100.00%\n",
            "Epoch [82/100] - Loss: 0.0327, Accuracy: 100.00%\n",
            "Epoch [83/100] - Loss: 0.0326, Accuracy: 100.00%\n",
            "Epoch [84/100] - Loss: 0.0325, Accuracy: 100.00%\n",
            "Epoch [85/100] - Loss: 0.0324, Accuracy: 100.00%\n",
            "Epoch [86/100] - Loss: 0.0332, Accuracy: 99.93%\n",
            "Epoch [87/100] - Loss: 0.0323, Accuracy: 100.00%\n",
            "Epoch [88/100] - Loss: 0.0322, Accuracy: 100.00%\n",
            "Epoch [89/100] - Loss: 0.0321, Accuracy: 100.00%\n",
            "Epoch [90/100] - Loss: 0.0323, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -7.6571183   9.084629  -12.424145 ]\n",
            " [ -4.5942636  -4.6858068   3.79142  ]\n",
            " [ 10.361221  -13.103657  -15.259748 ]\n",
            " [  9.117562   -8.351529  -16.95357  ]\n",
            " [ -8.424732   -8.320512    8.093389 ]]\n",
            "Epoch [91/100] - Loss: 0.0321, Accuracy: 100.00%\n",
            "Epoch [92/100] - Loss: 0.0320, Accuracy: 100.00%\n",
            "Epoch [93/100] - Loss: 0.0319, Accuracy: 100.00%\n",
            "Epoch [94/100] - Loss: 0.0319, Accuracy: 100.00%\n",
            "Epoch [95/100] - Loss: 0.0317, Accuracy: 100.00%\n",
            "Epoch [96/100] - Loss: 0.0317, Accuracy: 100.00%\n",
            "Epoch [97/100] - Loss: 0.0316, Accuracy: 100.00%\n",
            "Epoch [98/100] - Loss: 0.0315, Accuracy: 100.00%\n",
            "Epoch [99/100] - Loss: 0.0315, Accuracy: 100.00%\n",
            "Epoch [100/100] - Loss: 0.0313, Accuracy: 100.00%\n",
            "Sample Predictions: [[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Actual Labels: tensor([1, 2, 0, 0, 2], device='cuda:0')\n",
            "Sample Logits: [[ -7.4720383   8.594119   -9.845738 ]\n",
            " [-12.204455   -7.971427    9.1403265]\n",
            " [  7.871205   -7.7009354 -11.494435 ]\n",
            " [  5.328      -5.4705315 -11.3678055]\n",
            " [-13.619096   -9.400861    9.773075 ]]\n",
            "Final model weights saved to /content/drive/MyDrive/peak_encoder_transformer_pipeline/model_weights/latest_model.pth\n",
            "Batch 1: Validation Loss: 0.5372,\n",
            "Macro Precision: 0.8420, Macro Recall: 0.8132, Macro F1-score: 0.8226,\n",
            "Weighted Precision: 0.8414, Weighted Recall: 0.8133, Weighted F1-score: 0.8224,\n",
            "ROC-AUC: 0.9376, PR-AUC: 0.8923\n",
            "Class Distribution: Counter({np.int64(0): 81, np.int64(2): 80, np.int64(1): 80})\n",
            "Per-class metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Unmodified     0.7158    0.8395    0.7727        81\n",
            "   Oxidation     0.8750    0.7000    0.7778        80\n",
            "     Phospho     0.9351    0.9000    0.9172        80\n",
            "\n",
            "   micro avg     0.8305    0.8133    0.8218       241\n",
            "   macro avg     0.8420    0.8132    0.8226       241\n",
            "weighted avg     0.8414    0.8133    0.8224       241\n",
            " samples avg     0.8133    0.8133    0.8133       241\n",
            "\n",
            "‚ö†Ô∏è Score did not improve or excluded batch: 0.8915 (Best: 0.0000), Batch file: /content/drive/MyDrive/data/mini3/split_file_001.mgf\n",
            "Processing file: /content/drive/MyDrive/data/mini3/split_file_005.mgf\n",
            "Loaded model before training batch 2.\n",
            "Total skipped spectra: 0\n",
            "Labels: Counter({2: 400, 1: 400, 0: 396})\n",
            "Class Weights: [1.0084388  0.99583334 0.99583334]\n",
            "Epoch [1/100] - Loss: 0.5689, Accuracy: 86.72%\n",
            "Epoch [2/100] - Loss: 0.2683, Accuracy: 93.20%\n",
            "Epoch [3/100] - Loss: 0.1601, Accuracy: 96.13%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-755323117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     train_classifier_with_weights(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-342156878.py\u001b[0m in \u001b[0;36mtrain_classifier_with_weights\u001b[0;34m(model, data_tensors, labels, epochs, learning_rate, l1_lambda, save_path, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Add a flag to control if the model should be loaded before starting the loop\n",
        "load_latest_model_at_start = True  # Set this to True or False depending on whether you want to load the model at the start\n",
        "\n",
        "num_bins = 4500  #Number of bins of the 1D vector for the model\n",
        "num_classes = 3 #Number of modifications the model is trying to identify\n",
        "latent_size = 64\n",
        "dropout_prob = 0.22552320822488636\n",
        "epoch = 100\n",
        "learning_rate = 0.00017588221034432413\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "l1_lambda = 2.09073908962626e-07\n",
        "input_dir = \"/content/drive/MyDrive/data/mini3\"\n",
        "num_loops = 1 #number of loops performed over your dataset\n",
        "model_weights_dir = \"/content/drive/MyDrive/peak_encoder_transformer_pipeline/model_weights\"\n",
        "assume_observed = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pepmass_range = {'min': 500.00, 'max': 6000.00} #Fixed window for the normalization of the observed parent ion\n",
        "window_normaliation_size = 200.00 #set this to the window of m/z in which the intesitys are normalized\n",
        "min_score_threshold = 0.92  # Set this to the threshold you want the min score of your saved weights for the model\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Model Initialization\n",
        "model = EncoderTransformerClassifier(\n",
        "    latent_size=latent_size,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    dropout_prob=dropout_prob,\n",
        "    input_size=num_bins,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "# Model checkpoint path\n",
        "latest_model_path = f\"{model_weights_dir}/latest_model.pth\"\n",
        "\n",
        "# Dataset Handler Initialization\n",
        "handler = DatasetHandler(input_dir=input_dir, num_loops=num_loops)\n",
        "batch_counter = 1\n",
        "best_score = 0.0\n",
        "excluted_file_name = \"split_file_251.mgf\"\n",
        "\n",
        "# Load the latest model at the start if the flag is set to True\n",
        "if load_latest_model_at_start and os.path.exists(latest_model_path):\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(latest_model_path))\n",
        "        print(f\"Loaded model before starting the training loop.\")\n",
        "    except RuntimeError:\n",
        "        print(\"Parameter mismatch. Starting from scratch.\")\n",
        "else:\n",
        "    print(\"Skipping model load at the start.\")\n",
        "\n",
        "# Training Loop\n",
        "while True:\n",
        "    result = handler.get_next_file()\n",
        "    if result is None:\n",
        "        print(\"‚úÖ All spectra batches have been processed. Training loop complete.\")\n",
        "        break\n",
        "\n",
        "    spectra_batch, batch_file = result\n",
        "\n",
        "    # Load the latest model after each batch, as part of the training process\n",
        "    if os.path.exists(latest_model_path):\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(latest_model_path))\n",
        "            print(f\"Loaded model before training batch {batch_counter}.\")\n",
        "        except RuntimeError:\n",
        "            print(\"Parameter mismatch. Starting from scratch for this batch.\")\n",
        "\n",
        "    # Convert to features\n",
        "    feature_batch = combine_features(spectra_batch, pepmass_range, num_bins, window_normaliation_size, assume_observed)\n",
        "    if not feature_batch:\n",
        "        print(\"‚ö†Ô∏è Skipping empty feature batch.\")\n",
        "        continue\n",
        "\n",
        "    # Extract features and labels\n",
        "    spectra, parent_ions = zip(*feature_batch)\n",
        "    spectra = torch.tensor(np.array(spectra), dtype=torch.float32).to(device)\n",
        "    parent_ions = torch.tensor(np.array(parent_ions), dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(spectrum_label(spectra_batch), dtype=torch.long).to(device)\n",
        "\n",
        "    # Train/Val split\n",
        "    indices = np.arange(len(labels))\n",
        "    train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels.cpu().numpy(), random_state=42)\n",
        "\n",
        "    train_data = (spectra[train_idx], parent_ions[train_idx])\n",
        "    val_data = (spectra[val_idx], parent_ions[val_idx])\n",
        "    train_labels = labels[train_idx]\n",
        "    val_labels = labels[val_idx]\n",
        "\n",
        "    # Training\n",
        "    train_classifier_with_weights(\n",
        "        model, train_data, train_labels,\n",
        "        epochs=epoch,\n",
        "        learning_rate=learning_rate,\n",
        "        l1_lambda=l1_lambda,\n",
        "        save_path=latest_model_path,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    score = evaluate_model(model, val_data, val_labels, batch=batch_counter)\n",
        "\n",
        "    # Save the model only if the score is greater than both the best score and the minimum threshold\n",
        "    if (not math.isnan(score) and score > best_score and score >= min_score_threshold) and (excluted_file_name not in batch_file):\n",
        "        best_score = score\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_filename = f\"{model_weights_dir}/best_model_batch{batch_counter}_{timestamp}_score{best_score:.4f}.pth\"\n",
        "        torch.save(model.state_dict(), save_filename)\n",
        "        print(f\"‚úÖ Saved best model with improved Score: {best_score:.4f} at {save_filename}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Score did not improve or excluded batch: {score:.4f} (Best: {best_score:.4f}), Batch file: {batch_file}\")\n",
        "\n",
        "    batch_counter += 1\n",
        "\n",
        "# Final message\n",
        "print(\"\\nüéâ Training script completed successfully.\\nBest score achieved:\", best_score)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}